from tqdm import tqdm
import numpy as np
import pandas as pd
from itertools import accumulate
import matplotlib.pyplot as plt
import math
import os

import torch
import torch.nn as nn
import torch.nn.functional as F

import torchtext#; torchtext.disable_torchtext_deprecation_warning()
from torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors

from sklearn.manifold import TSNE

from torch.utils.data import DataLoader
import numpy as np
from torchtext.datasets import AG_NEWS
from IPython.display import Markdown as md
from tqdm import tqdm

from torch.utils.data.dataset import random_split,Dataset
from torchtext.data.functional import to_map_style_dataset
from sklearn.manifold import TSNE
import plotly.graph_objs as go

import pickle

from urllib.request import urlopen
import io

import tarfile
import tempfile

from torchtext.data.utils import get_tokenizer


from torchtext.vocab import vocab
from torch.nn.utils.rnn import pad_sequence

from Glove_override import GloVe_override
from GloVe_override2 import GloVe_override2

from IMDBDataset import IMDBDataset
from TextClassifier import TextClassifier

from sympy import Matrix, init_printing,Symbol
from numpy.linalg import qr,eig,inv,matrix_rank,inv, norm
from scipy.linalg import null_space
from sympy import Matrix, init_printing,Symbol

from urllib.request import urlopen
import io

from LinearWithLoRA import LinearWithLoRA

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

def plot(COST,ACC):
    fig, ax1 = plt.subplots()
    color = 'tab:red'
    ax1.plot(COST, color=color)
    ax1.set_xlabel('epoch', color=color)
    ax1.set_ylabel('total loss', color=color)
    ax1.tick_params(axis='y', color=color)

    ax2 = ax1.twinx()
    color = 'tab:blue'
    ax2.set_ylabel('accuracy', color=color)  # You already handled the x-label with ax1
    ax2.plot(ACC, color=color)
    ax2.tick_params(axis='y', color=color)
    fig.tight_layout()  # otherwise the right y-label is slightly clipped

    plt.show()

def save_list_to_file(lst, filename):
    """
    Save a list to a file using pickle serialization.

    Parameters:
        lst (list): The list to be saved.
        filename (str): The name of the file to save the list to.

    Returns:
        None
    """
    with open(filename, 'wb') as file:
        pickle.dump(lst, file)

def load_list_from_file(filename):
    """
    Load a list from a file using pickle deserialization.

    Parameters:
        filename (str): The name of the file to load the list from.

    Returns:
        list: The loaded list.
    """
    with open(filename, 'rb') as file:
        loaded_list = pickle.load(file)
    return loaded_list

tokenizer = get_tokenizer("basic_english")

def yield_tokens(data_iter):
    for  _,text in data_iter:
        yield tokenizer(text)

def text_pipeline(x):
  return vocab(tokenizer(x))

def label_pipeline(x):
   return int(x) 

def collate_batch(batch):
    label_list, text_list = [], []
    for _label, _text in batch:
        label_list.append(label_pipeline(_label))
        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))

    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = pad_sequence(text_list, batch_first=True)

    return label_list.to(device), text_list.to(device)

def predict(text, model, text_pipeline):
    with torch.no_grad():
        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)

        output = model(text)
        return imdb_label[output.argmax(1).item()]
    
def evaluate(dataloader, model, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for label, text in dataloader:
            label, text = label.to(device), text.to(device)
            outputs = model(text)
            _, predicted = torch.max(outputs.data, 1)
            total += label.size(0)
            correct += (predicted == label).sum().item()
    accuracy = 100 * correct / total
    return accuracy

def train_model(model, optimizer, criterion, train_dataloader, valid_dataloader, epochs=100, model_name="my_modeldrop"):
    cum_loss_list = []
    acc_epoch = []
    best_acc = 0
    file_name = model_name
    
    for epoch in tqdm(range(1, epochs + 1)):
        model.train()
        cum_loss = 0
        for _, (label, text) in enumerate(train_dataloader):            
            optimizer.zero_grad()
            predicted_label = model(text)
            loss = criterion(predicted_label, label)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
            optimizer.step()
            cum_loss += loss.item()
        #print("Loss:", cum_loss)
        cum_loss_list.append(cum_loss)
        acc_val = evaluate(valid_dataloader, model, device)
        acc_epoch.append(acc_val)
        
        if acc_val > best_acc:
            best_acc = acc_val
            print(f"New best accuracy: {acc_val:.4f}")
            #torch.save(model.state_dict(), f"{model_name}.pth")
    
    #save_list_to_file(cum_loss_list, f"{model_name}_loss.pkl")
    #save_list_to_file(acc_epoch, f"{model_name}_acc.pkl")

def plot_matrix_and_subspace(F):

    assert F.shape[0] == 3, "Matrix F must have rows equal to 3 for 3D visualization."
        
    ax = plt.figure().add_subplot(projection='3d')
    
    # Plot each column vector of F as a point and line from the origin
    for i in range(F.shape[1]):
        ax.quiver(0, 0, 0, F[0, i], F[1, i], F[2, i], color='blue', arrow_length_ratio=0.1, label=f'Column {i+1}')

    if F.shape[1] == 2:
        # Calculate the normal to the plane spanned by the columns of F if they are exactly two
        normal_vector = np.cross(F[:, 0], F[:, 1])
        # Plot the plane
        xx, yy = np.meshgrid(np.linspace(-3, 3, 10), np.linspace(-3, 3, 10))
        zz = (-normal_vector[0] * xx - normal_vector[1] * yy) / normal_vector[2] if normal_vector[2] != 0 else 0
        ax.plot_surface(xx, yy, zz, alpha=0.5, color='green', label='Spanned Plane')

        # Set plot limits and labels
        ax.set_xlim([-3, 3])
        ax.set_ylim([-3, 3])
        ax.set_zlim([-3, 3])
        ax.set_xlabel('$x_{1}$')
        ax.set_ylabel('$x_{2}$')
        ax.set_zlabel('$x_{3}$')
        #ax.legend()

        plt.show()
    
def plot_matrix_and_subspace(F):
    
    assert F.shape[0] == 3, "Matrix F must have 3 rows to represent 3D space."

    ax = plt.figure().add_subplot(projection='3d')
        
    # Plot each column vector of F
    for i in range(F.shape[1]):
        ax.quiver(0, 0, 0, F[0, i], F[1, i], F[2, i], color='blue', arrow_length_ratio=0.1, label=f'Column {i+1}')

    # Calculate the null space of the transpose of F
    normal_vector = null_space(F.T)
        
    # Check that the null space is 1-dimensional
    if normal_vector.shape[1] == 1:
        normal_vector = normal_vector[:, 0]  # Simplify the array to 1D
        
        # Create a meshgrid for the plane
        xx, yy = np.meshgrid(np.linspace(-3, 3, 10), np.linspace(-3, 3, 10))
        
        # Calculate corresponding z coordinates based on the plane equation ax + by + cz = 0
        zz = (-normal_vector[0] * xx - normal_vector[1] * yy) / normal_vector[2] if normal_vector[2] != 0 else 0
        ax.plot_surface(xx, yy, zz, alpha=0.5, color='green', label='Spanned Plane')
    else:
        print("The null space is not 1-dimensional, so a unique plane cannot be determined.")

    # Set plot limits and labels
    ax.set_xlim([-3, 3])
    ax.set_ylim([-3, 3])
    ax.set_zlim([-3, 3])
    ax.set_xlabel('X axis')
    ax.set_ylabel('Y axis')
    ax.set_zlabel('Z axis')
    #ax.legend()

    plt.show()

if __name__ == "__main__":

    try:
        glove_embedding = GloVe_override(name="6B", dim=100)
    except:
        try:
            glove_embedding = GloVe_override2(name="6B", dim=100)
        except:
            glove_embedding = GloVe(name="6B", dim=100)

    vocab = vocab(glove_embedding .stoi, 0,specials=('<unk>', '<pad>'))
    vocab.set_default_index(vocab["<unk>"])


    urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')
    tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))
    tempdir = tempfile.TemporaryDirectory()
    tar.extractall(tempdir.name)
    tar.close()


        
    root_dir = tempdir.name + '/' + 'imdb_dataset'
    train_iter = IMDBDataset(root_dir=root_dir, train=True)  # For training data
    test_iter = IMDBDataset(root_dir=root_dir, train=False)  # For test dataart=train_iter.pos_inx


    start=train_iter.pos_inx
    start=0
    for i in range(-10,10):
        print(train_iter[start+i])

    imdb_label = {0: " negative review", 1: "positive review"}
    print("imdb_label[1]: ", imdb_label[1])

    num_class = len(set([label for (label, text) in train_iter ]))
    print("num_class: ", num_class)

    vocab(["age","hello"])


    # Convert the training and testing iterators to map-style datasets.
    train_dataset = to_map_style_dataset(train_iter)
    test_dataset = to_map_style_dataset(test_iter)

    # Determine the number of samples to be used for training and validation (5% for validation).
    num_train = int(len(train_dataset) * 0.95)

    # Randomly split the training dataset into training and validation datasets using `random_split`.
    # The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.
    split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


    BATCH_SIZE = 64

    train_dataloader = DataLoader(
        split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
    )
    valid_dataloader = DataLoader(
        split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
    )

    label,seqence=next(iter(valid_dataloader ))
    print(label)
    print(seqence)

    model = TextClassifier(glove_embedding=glove_embedding,num_classes=num_class,freeze=False, device=device)
    model.to(device)

    model.eval()

    predicted_label = model(seqence)
    print(predicted_label.shape)

    predict("the is a good movie",model,text_pipeline )

    evaluate(test_dataloader , model, device)

    LEARNING_RATE=1

    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)

    model_name="model_imdb_freeze_true2"

    # for training model
    # train_model(model, optimizer, criterion, train_dataloader, valid_dataloader, epochs=2, model_name=model_name)

    cum_loss_list=load_list_from_file(model_name + "_loss.pkl")
    acc_epoch=load_list_from_file(model_name + "_acc.pkl")
    plot(cum_loss_list,acc_epoch)

    model.load_state_dict(torch.load(model_name + ".pth", map_location=device))
    model.eval()

    evaluate(test_dataloader , model, device)

    # Low-Rank Adaption
    init_printing()

    B=torch.tensor([[1,0],[0,1],[0,0]]).numpy()

    print(Matrix(B))
   
    plot_matrix_and_subspace(B)

    matrix_rank(B)

    B_=torch.tensor([[1,0],[-2,1],[0,1]]).numpy()
    plot_matrix_and_subspace(B_)
    print("rank of B",matrix_rank(B_))

    A=torch.tensor([[1,1,-1,1,0],[-2,2,2,0,1]]).numpy()
    print(Matrix(A))

    matrix_rank(A)

    C=B@A
    print(Matrix(C))

    print("rank of C",matrix_rank(C))
    plot_matrix_and_subspace(C)

    model_lora=TextClassifier(num_classes=4,freeze=False,glove_embedding=glove_embedding,device=device)
    model_lora.to(device)

    urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/uGC04Pom651hQs1XrZ0NsQ/my-model-freeze-false.pth')

    stream = io.BytesIO(urlopened.read())
    state_dict = torch.load(stream, map_location=device)
    model_lora.load_state_dict(state_dict)

    # Here, you freeze all layers:
    for parm in model_lora.parameters():
        parm.requires_grad=False
    
    print(model_lora)

    # replace the final layer with 2 classes
    model_lora.fc2=nn.Linear(in_features=128, out_features=2, bias=True).to(device)
    print(model_lora)

    # replace hidden layer with LoRA
    model_lora.fc1=LinearWithLoRA(model_lora.fc1,rank=2, alpha=0.1, device=device).to(device)
    print(model_lora.fc1)

    model_lora.to(device)

    LR=1
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model_lora.parameters(), lr=LR)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
    
    model_name="model_lora_final2"
    # for training model
    # train_model(model_lora,optimizer, criterion, train_dataloader, valid_dataloader, epochs=2, model_name=model_name)
    
    cum_loss_list=load_list_from_file(model_name + "_loss.pkl")
    acc_epoch=load_list_from_file(model_name + "_acc.pkl")
    plot(cum_loss_list,acc_epoch)

    model_lora.load_state_dict(torch.load(model_name + ".pth", map_location=device))
    model_lora.eval()

    evaluate(test_dataloader , model_lora, device)

    print(model_lora.fc1)

    B=model_lora.fc1.lora.B
    print("B",B)
    print("\n Number of elements in the tensor B",B.numel())
    torch.save(B, 'B.pth')

    A=model_lora.fc1.lora.A
    print("A",A)
    print("\n Number of elements in the tensor A",A.numel())
    torch.save(A, 'A.pth')


    print("\n Number of elements in the tensor A",model_lora.fc1.linear.weight.numel())

    alfa_=model_lora.fc1.lora.alpha
    torch.save(alfa_, 'alfa_.pth')
    torch.save(model_lora.fc2.state_dict(), 'out_layer.pth')

    A = torch.load('A.pth')
    print("A:",A.shape)

    B = torch.load('B.pth')
    print("B:",B.shape)

    alfa_ = torch.load('alfa_.pth')
    print("alfa_:",alfa_)

    output_layer=nn.Linear(in_features=128, out_features=2, bias=True)
    output_layer.load_state_dict(torch.load('out_layer.pth'))

    
    # Loading the model

    model_load_lora = TextClassifier(num_classes=4,freeze=False,glove_embedding=glove_embedding,device=device)
    model_load_lora.to(device)

    urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/uGC04Pom651hQs1XrZ0NsQ/my-model-freeze-false.pth')

    stream = io.BytesIO(urlopened.read())
    state_dict = torch.load(stream, map_location=device)
    model_load_lora.load_state_dict(state_dict)

    print(model_load_lora)

    model_load_lora.fc1=LinearWithLoRA(model_load_lora.fc1,rank=2, alpha=0.1, device=device)
    model_load_lora.fc2=nn.Linear(in_features=128, out_features=2, bias=True).to(device)
    
    model_load_lora.fc1.lora.A=A
    model_load_lora.fc1.lora.B=B
    model_load_lora.fc1.lora.alpha=alfa_ 
    model_load_lora.fc2=output_layer

    model_load_lora.to(device)
    model_load_lora.eval()

    evaluate(test_dataloader , model_load_lora, device)

    article="""This was a lacklustre movie with very little going for it. I was not impressed."""

    result = predict(article, model_load_lora, text_pipeline)

    print(result)
