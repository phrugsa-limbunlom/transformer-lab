import torch
import matplotlib.pyplot as plt
# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

import json

import numpy as np

from datasets import load_dataset, load_metric

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, BitsAndBytesConfig

from peft import LoraConfig, get_peft_model, TaskType, replace_lora_weights_loftq, prepare_model_for_kbit_training



def save_to_json(data, file_path):
    """
    Save a dictionary to a JSON file.

    Args:
        data (dict): The dictionary to save.
        file_path (str): The path to the JSON file.
    """
    with open(file_path, 'w') as json_file:
        json.dump(data, json_file, indent=4)
    print(f"Data successfully saved to {file_path}")
    
    
def load_from_json(file_path):
    """
    Load data from a JSON file.

    Args:
        file_path (str): The path to the JSON file.

    Returns:
        dict: The data loaded from the JSON file.
    """
    with open(file_path, 'r') as json_file:
        data = json.load(json_file)
    return data 

def preprocess_function(examples):
    return tokenizer(examples["text"], padding=True, truncation=True, max_length=512)  

def compute_metrics(eval_pred):
   load_accuracy = load_metric("accuracy", trust_remote_code=True)

  
   logits, labels = eval_pred
   predictions = np.argmax(logits, axis=-1)
   accuracy = load_accuracy.compute(predictions=predictions, references=labels)["accuracy"]

   return {"accuracy": accuracy}


if __name__ == "__main__":

    # Check CUDA availability but don't move model - it will be placed automatically
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print("Device:", device)

    imdb = load_dataset("imdb")

    # Display the structure of the dataset
    print("Dataset structure:")
    print(imdb)

    print(imdb.keys())

    # Explore and print a sample from the training set
    print("\nSample from the training set:")
    print(imdb['train'][0])


    train_labels = imdb['train']['label']
    unique_labels = set(train_labels)
    print("\nUnique labels in the dataset (class information):")
    print(unique_labels)

    class_names = {0: "negative", 1: "positive"}
    print(class_names)


    small_train_dataset = imdb["train"].shuffle(seed=42).select([i for i in list(range(50))])
    small_test_dataset = imdb["test"].shuffle(seed=42).select([i for i in list(range(50))])
    medium_train_dataset = imdb["train"].shuffle(seed=42).select([i for i in list(range(3000))])
    medium_test_dataset = imdb["test"].shuffle(seed=42).select([i for i in list(range(300))])

    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

    my_tokens=tokenizer(imdb['train'][0]['text'])

    # Print the tokenized input IDs
    print("Input IDs:", my_tokens['input_ids'])

    # Print the attention mask
    print("Attention Mask:", my_tokens['attention_mask'])

    # If token_type_ids is present, print it
    if 'token_type_ids' in my_tokens:
        print("Token Type IDs:", my_tokens['token_type_ids'])

    small_tokenized_train = small_train_dataset.map(preprocess_function, batched=True)
    small_tokenized_test = small_test_dataset.map(preprocess_function, batched=True)
    medium_tokenized_train = medium_train_dataset.map(preprocess_function, batched=True)
    medium_tokenized_test = medium_test_dataset.map(preprocess_function, batched=True)

    print(small_tokenized_train[49])


    # Configure Bits and Bytes
    config_bnb = BitsAndBytesConfig(
        load_in_4bit=True, # quantize the model to 4-bits when you load it
        bnb_4bit_quant_type="nf4", # use a special 4-bit data type for weights initialized from a normal distribution
        bnb_4bit_use_double_quant=True, # nested quantization scheme to quantize the already quantized weights
        bnb_4bit_compute_dtype=torch.bfloat16, # use bfloat16 for faster computation
        llm_int8_skip_modules=["classifier", "pre_classifier"] #  Don't convert the "classifier" and "pre_classifier" layers to 8-bit
    )

    id2label = {0: "NEGATIVE", 1: "POSITIVE"}
    label2id = dict((v,k) for k,v in id2label.items())

    model_qlora = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased",
                                                                    id2label=id2label,
                                                                    label2id=label2id,
                                                                    num_labels=2,
                                                                    quantization_config=config_bnb # load the model with the quantized version of the model
                                                                    )
    model_qlora = prepare_model_for_kbit_training(model_qlora)

    # set the model to be fine-tuned using QLoRA by converting the linear layers into LoRA layers
    lora_config = LoraConfig(
        task_type=TaskType.SEQ_CLS,  # Specify the task type as sequence classification
        r=8,  # Rank of the low-rank matrices
        lora_alpha=16,  # Scaling factor
        lora_dropout=0.1,  # Dropout rate  
        target_modules=['q_lin','k_lin','v_lin'] # which modules
    )
    
    # create a parameter efficient model
    peft_model_qlora = get_peft_model(model_qlora, lora_config)

    # reinitialize the LoRA weights using the LoftQ method
    replace_lora_weights_loftq(peft_model_qlora)

    print(peft_model_qlora)

    peft_model_qlora.print_trainable_parameters()

    training_args = TrainingArguments(
        output_dir="./results_qlora",
        num_train_epochs=10,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=64,
        learning_rate=2e-5,
        evaluation_strategy="epoch",
        weight_decay=0.01
    )

    trainer_qlora = Trainer(
    model=peft_model_qlora,
    args=training_args,
    train_dataset=medium_tokenized_train,
    eval_dataset=medium_tokenized_test,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics)

    trainer_qlora.train()

    trainer_qlora.save_model("./qlora_final_model")

    log_history_qlora = trainer_qlora.state.log_history

    print(log_history_qlora)

    get_metric_qlora = lambda metric, log_history_qlora: [log[metric] for log in log_history_qlora if metric in log]

    eval_accuracy_qlora=get_metric_qlora('eval_accuracy',log_history_qlora)
    eval_loss_qlora=get_metric_qlora('eval_loss',log_history_qlora)
    plt.plot(eval_accuracy_qlora,label='eval_accuracy')
    plt.plot(eval_loss_qlora,label='eval_loss')
    plt.xlabel("epoch")
    plt.legend()
    plt.show()

    # Save the plot to a file
    plt.savefig('qlora_training_metrics.png')
    print("Plot saved to qlora_training_metrics.png")
